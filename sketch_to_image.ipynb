{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmS-HgSWzO7I"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m747utFsfs7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqkPe9Vgsp15"
      },
      "outputs": [],
      "source": [
        "!ls \"/content/drive/MyDrive/Applied CV Project/Sketch to Image/datasets/sketchy_dataset/augmented_data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ma_NPabmuE_r"
      },
      "outputs": [],
      "source": [
        "!unzip -q '/content/drive/MyDrive/Applied CV Project/Sketch to Image/datasets/sketchy_dataset/augmented_data/data_aug_sketch.zip' -d sketch_dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrXcAxnHuxpo"
      },
      "outputs": [],
      "source": [
        "!ls sketch_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtwvh1X9ui4A"
      },
      "outputs": [],
      "source": [
        "sketch_dir = \"sketch_dataset/data_aug_sketch\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9Qb-bk0ydff"
      },
      "outputs": [],
      "source": [
        "!unzip -q '/content/drive/MyDrive/Applied CV Project/Sketch to Image/datasets/sketchy_dataset/augmented_data/data_aug_photo.zip' -d photo_dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs3SP_Gbyvk_"
      },
      "outputs": [],
      "source": [
        "photo_dir = \"photo_dataset/data_aug_photo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kbLbFZYwDhU"
      },
      "outputs": [],
      "source": [
        "def show_image(tensor_image, title=None):\n",
        "    tensor_image = tensor_image * 0.5 + 0.5\n",
        "    np_image = tensor_image.cpu().detach().numpy().transpose(1, 2, 0)\n",
        "    plt.imshow(np_image)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwyL6M2PtxfM"
      },
      "outputs": [],
      "source": [
        "class SketchToImageDataset(Dataset):\n",
        "    def __init__(self, sketch_dir, real_dir, transform=None, max_images=10000):\n",
        "        self.sketch_dir = Path(sketch_dir)\n",
        "        self.real_dir = Path(real_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "        self.sketch_filenames = sorted(os.listdir(sketch_dir))\n",
        "        self.real_filenames = sorted(os.listdir(real_dir))\n",
        "\n",
        "        self.sketch_filenames = self.sketch_filenames[:max_images]\n",
        "        self.real_filenames = self.real_filenames[:max_images]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sketch_filenames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sketch_filename = self.sketch_filenames[index]\n",
        "        real_filename = self.real_filenames[index]\n",
        "\n",
        "        sketch_path = self.sketch_dir / sketch_filename\n",
        "        real_path = self.real_dir / real_filename\n",
        "\n",
        "        sketch_image = Image.open(sketch_path).convert('L')\n",
        "        real_image = Image.open(real_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            sketch_image = self.transform(sketch_image)\n",
        "            real_image = self.transform(real_image)\n",
        "\n",
        "        return sketch_image, real_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIFjqdcNtzre"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "])\n",
        "\n",
        "dataset = SketchToImageDataset(sketch_dir, photo_dir, transform=transform)\n",
        "data_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=3, features=[64, 128, 256, 512]):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.encoder = nn.ModuleList()\n",
        "        current_channels = in_channels\n",
        "        for feature in features:\n",
        "            self.encoder.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(current_channels, feature, kernel_size=4, stride=2, padding=1),\n",
        "                    nn.BatchNorm2d(feature),\n",
        "                    nn.LeakyReLU(0.2)\n",
        "                )\n",
        "            )\n",
        "            current_channels = feature\n",
        "\n",
        "        self.decoder = nn.ModuleList()\n",
        "        reversed_features = list(reversed(features))\n",
        "\n",
        "        for i in range(len(reversed_features) - 1):\n",
        "            self.decoder.append(\n",
        "                nn.Sequential(\n",
        "                    nn.ConvTranspose2d(\n",
        "                        reversed_features[i] * 2 if i > 0 else reversed_features[i],\n",
        "                        reversed_features[i + 1],\n",
        "                        kernel_size=4,\n",
        "                        stride=2,\n",
        "                        padding=1\n",
        "                    ),\n",
        "                    nn.BatchNorm2d(reversed_features[i + 1]),\n",
        "                    nn.ReLU()\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.final_transpose = nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                reversed_features[-1] * 2,\n",
        "                features[0],\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(features[0]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.final_layer = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "        for layer in self.encoder:\n",
        "            x = layer(x)\n",
        "            skip_connections.append(x)\n",
        "\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx, layer in enumerate(self.decoder):\n",
        "            x = layer(x)\n",
        "\n",
        "            if idx < len(skip_connections) - 1:\n",
        "                skip_feature = skip_connections[idx + 1]\n",
        "                if x.shape[2:] == skip_feature.shape[2:]:\n",
        "                    x = torch.cat([x, skip_feature], dim=1)\n",
        "\n",
        "        x = self.final_transpose(x)\n",
        "        return self.final_layer(x)"
      ],
      "metadata": {
        "id": "zQvzxF_wSoOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=4, features=[64, 128, 256, 512]):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, features[0], kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(features[0], features[1], kernel_size=4, stride=2, padding=1),  #64 to 128\n",
        "            nn.BatchNorm2d(features[1]),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(features[1], features[2], kernel_size=4, stride=2, padding=1),  #128 to 256\n",
        "            nn.BatchNorm2d(features[2]),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(features[2], features[3], kernel_size=4, stride=2, padding=1),  #256 to 512\n",
        "            nn.BatchNorm2d(features[3]),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(features[3], 1, kernel_size=4, stride=1, padding=0),\n",
        "            nn.Sigmoid()  #Ensure output is in the [0, 1] range\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n"
      ],
      "metadata": {
        "id": "i9XrrFiKSpVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "\n",
        "# Loss functions\n",
        "adversarial_loss = nn.BCELoss()\n",
        "reconstruction_loss = nn.L1Loss()\n",
        "\n",
        "num_epochs = 75"
      ],
      "metadata": {
        "id": "Sp-Dn4EYSpf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (sketch, real_image) in enumerate(data_loader):\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        real_combined = torch.cat([sketch, real_image], dim=1)\n",
        "        real_output = discriminator(real_combined)\n",
        "        real_label = torch.ones_like(real_output)\n",
        "        loss_real = adversarial_loss(real_output, real_label)\n",
        "\n",
        "        fake_image = generator(sketch)\n",
        "        fake_combined = torch.cat([sketch, fake_image], dim=1)\n",
        "        fake_output = discriminator(fake_combined)\n",
        "        fake_label = torch.zeros_like(fake_output)\n",
        "        loss_fake = adversarial_loss(fake_output, fake_label)\n",
        "\n",
        "        # Discriminator loss\n",
        "        loss_D = (loss_real + loss_fake) / 2\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Training the generator\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        fake_image = generator(sketch)\n",
        "        fake_combined = torch.cat([sketch, fake_image], dim=1)\n",
        "        fake_output = discriminator(fake_combined)\n",
        "\n",
        "        generator_label = torch.ones_like(fake_output)\n",
        "        loss_adv = adversarial_loss(fake_output, generator_label)\n",
        "\n",
        "        loss_rec = reconstruction_loss(fake_image, real_image)\n",
        "\n",
        "        # Generator loss\n",
        "        lambda_recon = 100\n",
        "        loss_G = loss_adv + (lambda_recon * loss_rec)\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()"
      ],
      "metadata": {
        "id": "PkjrT8FFbzPt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}